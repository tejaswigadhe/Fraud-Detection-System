# Fraud Detection using Anomaly Detection & XGBoost

## Project Overview
Fraud detection is a critical challenge in the financial industry due to the high imbalance between fraudulent and legitimate transactions. This project explores **Anomaly Detection** techniques, including **Isolation Forest, Autoencoders, and XGBoost with SMOTE**, to identify fraudulent transactions effectively.

## Dataset
The dataset includes information about transactions made by credit cards in September 2013 by European cardholders. I will use the trained model to assess if a credit card transaction is legitimate or not. The dataset contains **284,807** transactions with the following distribution:
- **Legitimate Transactions (Class 0):** 99.83%
- **Fraudulent Transactions (Class 1):** 0.17%

üîó https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/creditcard.csv

Since fraud cases are extremely rare, **handling imbalanced data** is a key part of this project.

## üîç Approach & Models Used
### 1Ô∏è‚É£ **Isolation Forest (Unsupervised Learning)**
- Detects fraud based on how quickly an instance is isolated in decision trees.

### 2Ô∏è‚É£ **Autoencoder (Deep Learning)**
- A neural network trained to reconstruct normal transactions.
- High reconstruction error indicates possible fraud.

### 3Ô∏è‚É£ **XGBoost with SMOTE (Supervised Learning)**
- **XGBoost**: A gradient boosting algorithm.
- **SMOTE**: Synthetic Minority Over-sampling Technique to balance the dataset.

##  Handling Imbalanced Data
Since fraud cases are highly imbalanced, the following techniques were used:
-  **SMOTE** for oversampling fraudulent cases for XGBoost.
-  **Anomaly detection models** (Isolation Forest & Autoencoder), which do not require balanced data.

## Model Evaluation
Since accuracy alone is misleading for imbalanced data, we evaluated models using:
- **Precision-Recall (PR) Curve**
- **AUC-ROC Score**
- **F1-Score**

### **Precision-Recall Curve Results:**
- **Isolation Forest AUC = 0.135**
- **Autoencoder AUC = 0.170**
- **XGBoost AUC = 0.990** ‚úÖ

 **XGBoost outperformed anomaly detection models**, making it the best choice for fraud detection!

##  Feature Importance using SHAP
To understand which features contribute most to fraud detection, we used **SHAP values**. The top fraud indicators were:
- V4, V14, V8, and V12 have the most significant impact on fraud detection.
- Time and Amount are less influential compared to the engineered features (V1-V28).

##  Installation & Running the Project
### **1Ô∏è‚É£ Clone the Repository**
```bash
git clone https://github.com/tejaswigadhe/Fraud-Detection-System.git
cd Fraud-Detection-System
```

### **2Ô∏è‚É£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3Ô∏è‚É£ Run the Jupyter Notebook**
```bash
jupyter notebook
```
Open the notebook and run all cells to execute the project.


##  Contributing
Feel free to contribute by submitting a pull request! 

##  Contact
If you have any questions, reach out to me on **LinkedIn** or **GitHub**!

---
 **Star this repository** if you find it useful! ‚≠ê

